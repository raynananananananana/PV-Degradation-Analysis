{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b0c4d0e",
   "metadata": {},
   "source": [
    "This jupiter notebook combines multiple ways to automatically download PDFs based on metadata saved in a CSV file. The results from each download run are saved in separate folders, then combined at the end. Naming is standardized so that duplicate files are deleted. \n",
    "Please note that file and folder names must be adjusted based on the user's local naming systems. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd5849d",
   "metadata": {},
   "source": [
    "# run IEEE papers through IEEE code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa74580",
   "metadata": {},
   "source": [
    "not required - IEEE covered in API Keys code later in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e28473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openai\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68744f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file and save it as a .pkl file\n",
    "csv_path = \"PID_IEEE.csv\"\n",
    "pkl_path = \"PID_IEEE.pkl\"\n",
    "df_pid_ieee = pd.read_csv(csv_path)\n",
    "df_pid_ieee.to_pickle(pkl_path)\n",
    "df = df_pid_ieee\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8813f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_ieee_pdf(paper_id, download_path, paper_name):\n",
    "    pdf_url = 'http://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&isnumber=&arnumber={}'.format(paper_id)\n",
    "\n",
    "    # Define headers to mimic a browser request\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    # Send a request to download the PDF\n",
    "    response = requests.get(pdf_url, headers=headers)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        with open(f\"{download_path}{paper_name}.pdf\", \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"PDF downloaded successfully as {paper_name}\")\n",
    "    else:\n",
    "        print(\"Failed to download the PDF. Status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372eb57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ieee_paper_id(url):\n",
    "    \n",
    "    # The initial URL to open\n",
    "    initial_url = url\n",
    "\n",
    "    # Set up the WebDriver (Chrome)\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")  # Optional: Run in headless mode\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    ieee_paper_id = None\n",
    "    try:\n",
    "        # Step 1: Open the initial URL\n",
    "        driver.get(initial_url)\n",
    "        time.sleep(2)  # Wait for the page to load\n",
    "\n",
    "        # Step 2: Capture the current URL of the page after any redirects or interactions\n",
    "        current_url = driver.current_url\n",
    "\n",
    "        # Step 3: Use regex to find the number after 'document/'\n",
    "        match = re.search(r'document/(\\d+)', current_url)\n",
    "        if match:\n",
    "            ieee_paper_id = match.group(1)\n",
    "            print(\"Captured paper id:\", ieee_paper_id)\n",
    "\n",
    "    finally:\n",
    "        # Close the WebDriver\n",
    "        driver.quit()\n",
    "\n",
    "    return ieee_paper_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eb7b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the download directory exists\n",
    "os.makedirs(\"./dl_ieee_EID_names/\", exist_ok=True)\n",
    "\n",
    "for n in tqdm(np.arange(0, 77)):\n",
    "    inxid = int(inx_ieee[n])\n",
    "    doi = df['DOI'][inxid]\n",
    "    eid = str(df['EID'][inxid]).replace('.', '')  # Remove dot from EID\n",
    "    title = re.sub(r\"[\\/\\-?]\", \" \", df['Title'][inxid])\n",
    "    pdf_name = f\"{df['Type'][inxid]}-{df['Year'][inxid]}-{eid}\"\n",
    "    download_path = \"./dl_ieee/\"\n",
    "    url = f\"https://doi.org/{doi}\"\n",
    "    ieee_paper_id = get_ieee_paper_id(url)\n",
    "    download_ieee_pdf(ieee_paper_id, download_path, pdf_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b62b30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-run downloads for invalid files\n",
    "def get_unreadable_pdf_numbers(folder_path):\n",
    "    unreadable_numbers = []\n",
    "\n",
    "    # Traverse the folder for PDF files\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".pdf\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    # Try to open the PDF file\n",
    "                    reader = PdfReader(file_path)\n",
    "                    _ = len(reader.pages)  # Force reading to check if it works\n",
    "                except:\n",
    "                    # Extract the first number before the first hyphen\n",
    "                    match = re.match(r'(\\d+)-', file)\n",
    "                    if match:\n",
    "                        unreadable_numbers.append(int(match.group(1)))\n",
    "\n",
    "    return unreadable_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96671bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = './dl_ieee'\n",
    "unreadable_pdf_list = get_unreadable_pdf_numbers(folder_path)\n",
    "len(unreadable_pdf_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b32a340",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in tqdm(unreadable_pdf_list[20:50]):\n",
    "    inxid = inx_ieee[n]\n",
    "    doi = df['DOI'][inx_ieee[n]]\n",
    "    title = re.sub(r\"[\\/\\-?]\", \" \", df['Title'][inx_ieee[n]] )\n",
    "    pdf_name = str(n)+'-'+ str(inxid) + '-' + str(df['Year'][inx_ieee[n]]) + '-' + df['Type Code'][inx_ieee[n]] + '-' + title\n",
    "    download_path = \"./dl_ieee/\"\n",
    "    url = f\"https://doi.org/{doi}\"\n",
    "    ieee_paper_id = get_ieee_paper_id(url)\n",
    "    download_ieee_pdf(ieee_paper_id, download_path, pdf_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bd69b1",
   "metadata": {},
   "source": [
    "# run Elsevier downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b14bc5b",
   "metadata": {},
   "source": [
    "not required - IEEE covered in API Keys code later in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1518457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openai\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "import os\n",
    "import webbrowser\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pyautogui\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c714cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file and save it as a pickle file\n",
    "import os\n",
    "\n",
    "csv_path = \"/Users/Rayna/Downloads/HackingMat/PID_elsevier.csv\"\n",
    "if os.path.exists(csv_path):\n",
    "\tdf_elsevier = pd.read_csv(csv_path)\n",
    "\tdf_elsevier.to_pickle(\"PID_elsevier.pkl\")\n",
    "\tprint(\"CSV loaded and pickle file saved.\")\n",
    "else:\n",
    "\tprint(f\"File '{csv_path}' not found. Please check the file path.\")\n",
    "\n",
    "df = pd.read_pickle('PID_elsevier.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caaa79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you only want to filter by Publisher in a list of Elsevier names\n",
    "inx_elsevier = df.index[df['Publisher'].isin(['Elsevier', 'Elsevier B.V.', 'Elsevier Ltd', 'Elsevier GmbH'])]\n",
    "\n",
    "df.iloc[inx_elsevier]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7803ff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is to know the already downloaded papers \n",
    "\n",
    "def extract_second_number_from_pdfs(folder_path):\n",
    "    \"\"\"\n",
    "    Finds all PDF files in a folder, extracts the second number from the file title,\n",
    "    and returns a list of those numbers.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): The path to the folder containing the PDF files.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of extracted second numbers (as integers), or an empty list if no matching files are found.\n",
    "    \"\"\"\n",
    "    pdf_numbers = []\n",
    "    try:\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.lower().endswith(\".pdf\"):\n",
    "                match = re.match(r'^\\d+-(\\d+)-\\d+-.+', filename)  # Uses regex to find pattern\n",
    "                if match:\n",
    "                    second_number = int(match.group(1))\n",
    "                    pdf_numbers.append(second_number)\n",
    "        pdf_numbers.sort()  # Sort the list in ascending order\n",
    "        return pdf_numbers\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Folder '{folder_path}' not found.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10187f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inx_download = inx_elsevier\n",
    "category_path = './dl_elsevier/'\n",
    "existing_paper_ids = extract_second_number_from_pdfs(category_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873119c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_dir = os.path.expanduser(\"~/Downloads/HackingMat\")\n",
    "folder_path = os.path.join(base_dir, \"dl_elsevier_PID\")\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "print(f\"Folder created at: {folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920e6388",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = '9cfa907a1d1e50bab0fe344be2646890' #replace with your API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f7b9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Make sure this folder exists\n",
    "download_folder = os.path.join(base_dir, \"dl_elsevier_EID\")\n",
    "os.makedirs(download_folder, exist_ok=True)\n",
    "\n",
    "for n, inxid in enumerate(tqdm(inx_elsevier)):\n",
    "    if inxid not in existing_paper_ids:\n",
    "        doi = df.at[inxid, 'DOI']\n",
    "        if isinstance(doi, str):\n",
    "            # Extract metadata\n",
    "            paper_type = df.at[inxid, 'Type']\n",
    "            year = df.at[inxid, 'Year']\n",
    "            eid = str(df.at[inxid, 'EID']).replace('.', '')  # Remove dot from EID\n",
    "            \n",
    "            # Construct clean filename\n",
    "            filename = f\"{paper_type}_{year}_{eid}.pdf\"\n",
    "            filename_path = os.path.join(download_folder, filename)\n",
    "\n",
    "            # Build the download URL\n",
    "            url = f\"https://api.elsevier.com/content/article/doi/{doi}?apiKey={api}&httpAccept=application%2Fpdf\"\n",
    "\n",
    "            try:\n",
    "                response = requests.get(url, stream=True)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                if response.headers.get('x-els-status') == 'OK' and response.headers.get('Content-Type') == 'application/pdf':\n",
    "                    with open(filename_path, 'wb') as f:\n",
    "                        f.write(response.content)\n",
    "                    print(f\"PDF saved as: {filename}\")\n",
    "                else:\n",
    "                    print(f\"Skipped: {filename} - Not a PDF or status not OK\")\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error downloading PDF for DOI {doi}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e773b10",
   "metadata": {},
   "source": [
    "# use fulltext_article_downloader for other publishers with APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d304be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set API keys for publishers that provide them\n",
    "import os\n",
    "os.environ[\"ELSEVIER_API_KEY\"] = \"your api key\"\n",
    "os.environ[\"SPRINGER_API_KEY\"] = \"your api key\"\n",
    "os.environ[\"WILEY_API_KEY\"] = \"your api key\"\n",
    "os.environ[\"IEEE_API_KEY\"] = \"your api key\"\n",
    "os.environ[\"UNPAYWALL_EMAIL\"] = \"your email address\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b458226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fulltext_article_downloader import bulk_download_articles  # assuming this is imported correctly\n",
    "from tqdm import tqdm\n",
    "\n",
    "# set file paths, load CSV and study type\n",
    "csv_path = \"path to CSV with metadata\"  # Update to your file\n",
    "output_base = \"path to output folder\"  # Main folder for downloaded PDFs\n",
    "study_type = os.path.splitext(os.path.basename(csv_path))[0]  # e.g., \"PID\"\n",
    "df = pd.read_csv(csv_path)\n",
    "doi_list = df['DOI'].dropna().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba52ed8",
   "metadata": {},
   "source": [
    "naming format for files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200f4f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename mapping\n",
    "doi_to_filename = {}\n",
    "\n",
    "def get_metadata_for_doi(row):\n",
    "    year = str(row.get('Year', 'Unknown'))\n",
    "    eid = str(row.get('EID', 'Unknown')).replace('.', '')\n",
    "    type = str(row.get('Type', 'Unknown'))\n",
    "    return f\"{type}_{year}_{eid}\"\n",
    "\n",
    "if {'DOI', 'Year', 'Publisher', 'Title', 'EID', 'Type'}.issubset(df.columns):\n",
    "    for _, row in df.iterrows():\n",
    "        doi = row['DOI']\n",
    "        if pd.notna(doi):\n",
    "            doi_to_filename[doi] = get_metadata_for_doi(row)\n",
    "else:\n",
    "    raise ValueError(\"CSV is missing one or more required columns: DOI, Year, Title, Publisher, EID, Type\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eb0274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set download file\n",
    "output_dir = os.path.join(output_base, f\"{study_type}_papers\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "log_file = os.path.join(output_dir, f\"{study_type}_download.log\")\n",
    "\n",
    "# count PDFs before download\n",
    "pdf_count_before = sum(\n",
    "    f.lower().endswith('.pdf')\n",
    "    for root, _, files in os.walk(output_dir)\n",
    "    for f in files\n",
    ")\n",
    "\n",
    "print(f\"\\nDownloading papers for {study_type} ({len(doi_list)} DOIs)...\")\n",
    "results = bulk_download_articles(\n",
    "    doi_list,\n",
    "    output_dir=output_dir,\n",
    "    log_file=log_file,\n",
    "    sleep=0.2,\n",
    ")\n",
    "\n",
    "# correctly name files\n",
    "for filename in os.listdir(output_dir):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        original_path = os.path.join(output_dir, filename)\n",
    "\n",
    "        # Try to extract the DOI\n",
    "        doi_guess = filename.replace(\".pdf\", \"\").replace(\"_\", \"/\")\n",
    "        matched_doi = None\n",
    "\n",
    "        if doi_guess in doi_to_filename:\n",
    "            matched_doi = doi_guess\n",
    "        else:\n",
    "            for doi in doi_to_filename:\n",
    "                if doi.replace(\"/\", \"_\") in filename or doi in filename:\n",
    "                    matched_doi = doi\n",
    "                    break\n",
    "\n",
    "        if matched_doi and matched_doi in doi_to_filename:\n",
    "            new_name_raw = doi_to_filename[matched_doi]\n",
    "            clean_name = \"\".join(c for c in new_name_raw if c.isalnum() or c in \" ._-\")\n",
    "            clean_name = clean_name.strip().replace(\" \", \"_\") + \".pdf\"\n",
    "            new_path = os.path.join(output_dir, clean_name)\n",
    "\n",
    "            try:\n",
    "                os.rename(original_path, new_path)\n",
    "                print(f\"Renamed: {filename} → {clean_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Rename failed for {filename}: {e}\")\n",
    "        else:\n",
    "            print(f\"Could not match filename to DOI: {filename}\")\n",
    "\n",
    "# log failed DOIs\n",
    "failed_dois = []\n",
    "if results:\n",
    "    for doi, status in results.items():\n",
    "        pdf_name_guess = f\"{doi.replace('/', '_')}.pdf\"\n",
    "        file_path = os.path.join(output_dir, pdf_name_guess)\n",
    "        if not status or not os.path.exists(file_path):\n",
    "            failed_row = df[df['DOI'] == doi]\n",
    "            if not failed_row.empty:\n",
    "                failed_dois.append(failed_row)\n",
    "\n",
    "if failed_dois:\n",
    "    failed_df = pd.concat(failed_dois, ignore_index=True)\n",
    "    failed_csv_path = os.path.join(output_base, f\"failed_{study_type}.csv\")\n",
    "    failed_df.to_csv(failed_csv_path, index=False)\n",
    "    print(f\"Saved {len(failed_df)} failed DOIs to: {failed_csv_path}\")\n",
    "\n",
    "# count PDFs\n",
    "pdf_count_after = sum(\n",
    "    f.lower().endswith('.pdf')\n",
    "    for root, _, files in os.walk(output_dir)\n",
    "    for f in files\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal PDF files in '{output_dir}': {pdf_count_after}\")\n",
    "print(f\"New PDFs downloaded: {pdf_count_after - pdf_count_before}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1812fa",
   "metadata": {},
   "source": [
    "# search with Selenium as a final backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcd1997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d155d56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set paths to CSV & download directory\n",
    "# repeat by changing CSV to read (i.e. failed_glass) - use CSVs of DOIs that failed to download through API code\n",
    "csv_path = 'path to your csv file with metadata' \n",
    "download_dir = 'path to desired output folder'\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "# load CSV\n",
    "df = pd.read_csv(csv_path)\n",
    "df = df[df['EID'].notna()]  # Remove rows without DOI\n",
    "\n",
    "# track failed download DOIs\n",
    "failed_dois = []\n",
    "\n",
    "# set chrome options & start browser\n",
    "chrome_options = Options()\n",
    "chrome_options.add_experimental_option(\"prefs\", {\n",
    "    \"download.default_directory\": download_dir,\n",
    "    \"download.prompt_for_download\": False,\n",
    "    \"plugins.always_open_pdf_externally\": True\n",
    "})\n",
    "chrome_options.add_argument(\"--start-maximized\")\n",
    "chrome_options.add_argument(\"--headless=new\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "# download PDFs based on DOI\n",
    "for index, row in df.iterrows():\n",
    "    doi = row['DOI']\n",
    "    year = str(row.get('Year', 'Unknown'))\n",
    "    publisher = str(row.get('Publisher', 'Unknown')).replace('/', '-')\n",
    "    eid = str(row.get('EID', 'Unknown')).replace('.', '')\n",
    "    type = str(row.get('Type', 'Unknown'))\n",
    "    title = str(row.get('Title', 'no_title'))[:50].replace('/', '-').replace('\\\\', '-').replace(' ', '_')\n",
    "    url = f\"https://doi.org/{doi}\"\n",
    "\n",
    "    print(f\"Opening: {url}\")\n",
    "\n",
    "    # Record existing files to detect new ones after download\n",
    "    before_files = set(os.listdir(download_dir))\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(5)  # Let redirect happen\n",
    "\n",
    "        # Find a link with 'pdf' in href\n",
    "        pdf_button = driver.find_element(By.XPATH, \"//a[contains(@href, 'pdf')]\")\n",
    "        pdf_link = pdf_button.get_attribute(\"href\")\n",
    "\n",
    "        if pdf_link:\n",
    "            driver.get(pdf_link)\n",
    "            print(f\"Downloading PDF for: {doi}\")\n",
    "            time.sleep(10)  # Adjust based on your connection\n",
    "\n",
    "            # Detect new file\n",
    "            after_files = set(os.listdir(download_dir))\n",
    "            new_files = after_files - before_files\n",
    "            pdf_file = None\n",
    "            for file in new_files:\n",
    "                if file.endswith('.pdf'):\n",
    "                    pdf_file = file\n",
    "                    break\n",
    "\n",
    "            if pdf_file:\n",
    "                new_name = f\"{type}_{year}_{eid}.pdf\"\n",
    "                clean_name = \"\".join(c for c in new_name if c.isalnum() or c in \" ._-\").strip().replace(\" \", \"_\")\n",
    "                new_path = os.path.join(download_dir, clean_name)\n",
    "                original_path = os.path.join(download_dir, pdf_file)\n",
    "                shutil.move(original_path, new_path)\n",
    "                print(f\"Renamed to: {clean_name}\")\n",
    "            else:\n",
    "                print(f\"No new PDF detected for {doi}\")\n",
    "                failed_dois.append(row)\n",
    "\n",
    "        else:\n",
    "            print(f\"No PDF link found for {doi}\")\n",
    "            failed_dois.append(row)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {doi}: {e}\")\n",
    "        failed_dois.append(row)\n",
    "\n",
    "# browser cleanup\n",
    "driver.quit()\n",
    "\n",
    "# write failed download DOIs to a new CSV\n",
    "if failed_dois:\n",
    "    failed_df = pd.DataFrame(failed_dois)\n",
    "    failed_csv_path = os.path.join(os.path.dirname(csv_path), \"selenium_failed_altypes.csv\")\n",
    "    failed_df.to_csv(failed_csv_path, index=False)\n",
    "    print(f\"Saved {len(failed_df)} failed DOIs to: {failed_csv_path}\")\n",
    "else:\n",
    "    print(\"All PDFs downloaded successfully!\")\n",
    "\n",
    "print(\"Done downloading PDFs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d2bf32",
   "metadata": {},
   "source": [
    "# count and combine PDFs, delete duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb8ce2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the folder names are changed to match your local directory\n",
    "import os\n",
    "\n",
    "base_dir = \"folder with all PDFs\"\n",
    "dirs = [\n",
    "    \"auto_downloads/PID_papers\",\n",
    "    \"PID Papers by EID/dl_elsevier_EID\",\n",
    "    \"PID Papers by EID/dl_ieee_EID\"\n",
    "] # subfolders (if applicable)\n",
    "\n",
    "pdf_counts = {}\n",
    "for d in dirs:\n",
    "    full_path = os.path.join(base_dir, d)\n",
    "    count = sum(\n",
    "        f.lower().endswith('.pdf')\n",
    "        for root, _, files in os.walk(full_path)\n",
    "        for f in files\n",
    "    )\n",
    "    pdf_counts[d] = count\n",
    "\n",
    "for d, count in pdf_counts.items():\n",
    "    print(f\"{d}: {count} PDF files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775aac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Define source and destination directories\n",
    "src_dirs = [\n",
    "    \"directories above\"\n",
    "]\n",
    "dst_dir = \"output directory for all PDFs, accounting for no duplicates\"\n",
    "os.makedirs(dst_dir, exist_ok=True)\n",
    "\n",
    "# Helper to generate unique filename if duplicate exists\n",
    "def get_unique_filename(dst_dir, filename):\n",
    "    base, ext = os.path.splitext(filename)\n",
    "    counter = 1\n",
    "    new_filename = filename\n",
    "    while os.path.exists(os.path.join(dst_dir, new_filename)):\n",
    "        new_filename = f\"{base}({counter}){ext}\"\n",
    "        counter += 1\n",
    "    return new_filename\n",
    "\n",
    "# Copy files, handling duplicates by appending a number\n",
    "for src in src_dirs:\n",
    "    for fname in os.listdir(src):\n",
    "        if fname.lower().endswith('.pdf'):\n",
    "            src_path = os.path.join(src, fname)\n",
    "            unique_fname = get_unique_filename(dst_dir, fname)\n",
    "            dst_path = os.path.join(dst_dir, unique_fname)\n",
    "            shutil.copy2(src_path, dst_path)\n",
    "\n",
    "# Count PDFs in the final folder\n",
    "pdf_count = sum(\n",
    "    f.lower().endswith('.pdf')\n",
    "    for f in os.listdir(dst_dir)\n",
    ")\n",
    "print(f\"Total PDF files in '{dst_dir}': {pdf_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
